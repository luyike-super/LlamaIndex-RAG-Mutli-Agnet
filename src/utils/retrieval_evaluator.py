"""
æ£€ç´¢æ•ˆæœè¯„ä¼°å·¥å…·
"""
import time
import logging
from typing import List, Dict, Any, Tuple
from llama_index.core.schema import NodeWithScore
from config.config_rag import RETRIEVAL_CONFIG

logger = logging.getLogger(__name__)

class RetrievalEvaluator:
    """æ£€ç´¢æ•ˆæœè¯„ä¼°å·¥å…·"""
    
    def __init__(self):
        self.evaluation_results = []
    
    def evaluate_query(
        self, 
        query: str, 
        query_engine: Any,
        expected_keywords: List[str] = None,
        expected_docs: List[str] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæŸ¥è¯¢çš„æ£€ç´¢æ•ˆæœ
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            query_engine: æŸ¥è¯¢å¼•æ“
            expected_keywords: æœŸæœ›åœ¨ç»“æœä¸­å‡ºç°çš„å…³é”®è¯
            expected_docs: æœŸæœ›æ£€ç´¢åˆ°çš„æ–‡æ¡£ID
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            response = query_engine.query(query)
            query_time = time.time() - start_time
            
            # æå–æ£€ç´¢åˆ°çš„èŠ‚ç‚¹
            retrieved_nodes = getattr(response, 'source_nodes', [])
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            metrics = self._calculate_metrics(
                query=query,
                retrieved_nodes=retrieved_nodes,
                response_text=str(response),
                expected_keywords=expected_keywords,
                expected_docs=expected_docs,
                query_time=query_time
            )
            
            # ä¿å­˜è¯„ä¼°ç»“æœ
            evaluation_result = {
                "query": query,
                "response": str(response),
                "metrics": metrics,
                "retrieved_count": len(retrieved_nodes),
                "query_time": query_time,
                "timestamp": time.time()
            }
            
            self.evaluation_results.append(evaluation_result)
            
            return evaluation_result
            
        except Exception as e:
            logger.error(f"è¯„ä¼°æŸ¥è¯¢ '{query}' æ—¶å‡ºé”™: {e}")
            return {
                "query": query,
                "error": str(e),
                "query_time": time.time() - start_time
            }
    
    def _calculate_metrics(
        self,
        query: str,
        retrieved_nodes: List[NodeWithScore],
        response_text: str,
        expected_keywords: List[str] = None,
        expected_docs: List[str] = None,
        query_time: float = 0
    ) -> Dict[str, float]:
        """è®¡ç®—æ£€ç´¢è¯„ä¼°æŒ‡æ ‡"""
        
        metrics = {
            "retrieval_count": len(retrieved_nodes),
            "avg_score": 0.0,
            "min_score": 0.0,
            "max_score": 0.0,
            "keyword_coverage": 0.0,
            "doc_recall": 0.0,
            "response_length": len(response_text),
            "query_time": query_time
        }
        
        if not retrieved_nodes:
            return metrics
        
        # è®¡ç®—åˆ†æ•°ç»Ÿè®¡
        scores = [node.score for node in retrieved_nodes if node.score is not None]
        if scores:
            metrics["avg_score"] = sum(scores) / len(scores)
            metrics["min_score"] = min(scores)
            metrics["max_score"] = max(scores)
        
        # è®¡ç®—å…³é”®è¯è¦†ç›–ç‡
        if expected_keywords:
            metrics["keyword_coverage"] = self._calculate_keyword_coverage(
                response_text, expected_keywords
            )
        
        # è®¡ç®—æ–‡æ¡£å¬å›ç‡
        if expected_docs:
            metrics["doc_recall"] = self._calculate_doc_recall(
                retrieved_nodes, expected_docs
            )
        
        return metrics
    
    def _calculate_keyword_coverage(
        self, 
        response_text: str, 
        expected_keywords: List[str]
    ) -> float:
        """è®¡ç®—å…³é”®è¯è¦†ç›–ç‡"""
        if not expected_keywords:
            return 0.0
        
        response_lower = response_text.lower()
        found_keywords = 0
        
        for keyword in expected_keywords:
            if keyword.lower() in response_lower:
                found_keywords += 1
        
        return found_keywords / len(expected_keywords)
    
    def _calculate_doc_recall(
        self, 
        retrieved_nodes: List[NodeWithScore], 
        expected_docs: List[str]
    ) -> float:
        """è®¡ç®—æ–‡æ¡£å¬å›ç‡"""
        if not expected_docs:
            return 0.0
        
        retrieved_doc_ids = set()
        for node in retrieved_nodes:
            if hasattr(node.node, 'ref_doc_id') and node.node.ref_doc_id:
                retrieved_doc_ids.add(node.node.ref_doc_id)
        
        found_docs = len(retrieved_doc_ids.intersection(set(expected_docs)))
        return found_docs / len(expected_docs)
    
    def run_evaluation_suite(
        self, 
        test_queries: List[Dict[str, Any]], 
        query_engine: Any
    ) -> Dict[str, Any]:
        """
        è¿è¡Œè¯„ä¼°æµ‹è¯•å¥—ä»¶
        
        Args:
            test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«queryã€expected_keywordsç­‰
            query_engine: è¦è¯„ä¼°çš„æŸ¥è¯¢å¼•æ“
            
        Returns:
            ç»¼åˆè¯„ä¼°ç»“æœ
        """
        logger.info(f"å¼€å§‹è¿è¡Œè¯„ä¼°å¥—ä»¶ï¼Œå…± {len(test_queries)} ä¸ªæµ‹è¯•æŸ¥è¯¢")
        
        results = []
        for test_case in test_queries:
            query = test_case["query"]
            expected_keywords = test_case.get("expected_keywords", [])
            expected_docs = test_case.get("expected_docs", [])
            
            result = self.evaluate_query(
                query=query,
                query_engine=query_engine,
                expected_keywords=expected_keywords,
                expected_docs=expected_docs
            )
            results.append(result)
        
        # è®¡ç®—ç»¼åˆæŒ‡æ ‡
        summary = self._calculate_summary_metrics(results)
        
        logger.info("è¯„ä¼°å¥—ä»¶è¿è¡Œå®Œæˆ")
        
        return {
            "individual_results": results,
            "summary": summary,
            "total_queries": len(test_queries),
            "successful_queries": len([r for r in results if "error" not in r])
        }
    
    def _calculate_summary_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:
        """è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡"""
        valid_results = [r for r in results if "error" not in r and "metrics" in r]
        
        if not valid_results:
            return {}
        
        # æå–æ‰€æœ‰æŒ‡æ ‡
        all_metrics = [r["metrics"] for r in valid_results]
        
        summary = {}
        metric_names = [
            "retrieval_count", "avg_score", "keyword_coverage", 
            "doc_recall", "response_length", "query_time"
        ]
        
        for metric_name in metric_names:
            values = [m.get(metric_name, 0) for m in all_metrics]
            if values:
                summary[f"avg_{metric_name}"] = sum(values) / len(values)
                summary[f"max_{metric_name}"] = max(values)
                summary[f"min_{metric_name}"] = min(values)
        
        return summary
    
    def print_evaluation_report(self, evaluation_result: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print("\n" + "="*50)
        print("æ£€ç´¢æ•ˆæœè¯„ä¼°æŠ¥å‘Š")
        print("="*50)
        
        if "summary" in evaluation_result:
            print("\nğŸ“Š ç»¼åˆæŒ‡æ ‡:")
            summary = evaluation_result["summary"]
            
            print(f"  â€¢ å¹³å‡æ£€ç´¢æ•°é‡: {summary.get('avg_retrieval_count', 0):.1f}")
            print(f"  â€¢ å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {summary.get('avg_avg_score', 0):.3f}")
            print(f"  â€¢ å¹³å‡å…³é”®è¯è¦†ç›–ç‡: {summary.get('avg_keyword_coverage', 0)*100:.1f}%")
            print(f"  â€¢ å¹³å‡æ–‡æ¡£å¬å›ç‡: {summary.get('avg_doc_recall', 0)*100:.1f}%")
            print(f"  â€¢ å¹³å‡å“åº”é•¿åº¦: {summary.get('avg_response_length', 0):.0f} å­—ç¬¦")
            print(f"  â€¢ å¹³å‡æŸ¥è¯¢æ—¶é—´: {summary.get('avg_query_time', 0):.2f} ç§’")
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"  â€¢ æ€»æŸ¥è¯¢æ•°: {evaluation_result.get('total_queries', 0)}")
        print(f"  â€¢ æˆåŠŸæŸ¥è¯¢æ•°: {evaluation_result.get('successful_queries', 0)}")
        
        # æ‰“å°ä¸ªåˆ«æŸ¥è¯¢ç»“æœ
        if "individual_results" in evaluation_result:
            print(f"\nğŸ“ è¯¦ç»†ç»“æœ:")
            for i, result in enumerate(evaluation_result["individual_results"][:5], 1):
                if "error" not in result:
                    metrics = result.get("metrics", {})
                    print(f"  {i}. æŸ¥è¯¢: {result['query'][:50]}...")
                    print(f"     æ£€ç´¢æ•°é‡: {metrics.get('retrieval_count', 0)}")
                    print(f"     å¹³å‡åˆ†æ•°: {metrics.get('avg_score', 0):.3f}")
                    print(f"     æŸ¥è¯¢æ—¶é—´: {metrics.get('query_time', 0):.2f}s")

def create_test_queries() -> List[Dict[str, Any]]:
    """åˆ›å»ºæµ‹è¯•æŸ¥è¯¢é›†åˆ"""
    return [
        {
            "query": "ä½¿ç”¨ç¡¬ä»¶æ§åˆ¶æ¨¡å¼ï¼Œè¾“å‡ºç±»å‹è®¾ç½®ä¸º HCSL",
            "expected_keywords": ["ç¡¬ä»¶æ§åˆ¶", "HCSL", "è¾“å‡ºç±»å‹", "driver_type", "output_drive_low"],
            "description": "ç¡¬ä»¶æ§åˆ¶å’Œè¾“å‡ºç±»å‹é…ç½®"
        },
        {
            "query": "æ—¶é’Ÿè¾“å‡ºé¢‘ç‡å¦‚ä½•é…ç½®",
            "expected_keywords": ["æ—¶é’Ÿ", "é¢‘ç‡", "é…ç½®", "è¾“å‡º", "clock"],
            "description": "æ—¶é’Ÿé¢‘ç‡é…ç½®"
        },
        {
            "query": "èŠ¯ç‰‡çš„ä¾›ç”µç”µå‹èŒƒå›´",
            "expected_keywords": ["ä¾›ç”µ", "ç”µå‹", "èŒƒå›´", "èŠ¯ç‰‡", "VDD"],
            "description": "ä¾›ç”µç”µå‹è§„æ ¼"
        },
        {
            "query": "SPIæ¥å£å¦‚ä½•ä½¿ç”¨",
            "expected_keywords": ["SPI", "æ¥å£", "ä½¿ç”¨", "é€šä¿¡", "é…ç½®"],
            "description": "SPIæ¥å£ä½¿ç”¨æ–¹æ³•"
        },
        {
            "query": "æ¸©åº¦èŒƒå›´å’Œå·¥ä½œæ¡ä»¶",
            "expected_keywords": ["æ¸©åº¦", "èŒƒå›´", "å·¥ä½œæ¡ä»¶", "ç¯å¢ƒ", "è§„æ ¼"],
            "description": "å·¥ä½œç¯å¢ƒè§„æ ¼"
        }
    ] 