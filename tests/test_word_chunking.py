"""
Wordæ–‡æ¡£åˆ‡åˆ†åŠŸèƒ½æµ‹è¯•
"""
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

def test_word_parser_configuration():
    """æµ‹è¯•Wordè§£æå™¨é…ç½®æ˜¯å¦æ­£ç¡®"""
    print("ğŸ”§ æµ‹è¯•Wordè§£æå™¨é…ç½®...")
    
    try:
        from config.config_rag import CHUNKING_CONFIG
        from src.node_parser.word_parser import WordNodeParser
        
        # æ£€æŸ¥é…ç½®
        docx_config = CHUNKING_CONFIG.get("docx", {})
        print(f"ğŸ“‹ docxé…ç½®: {docx_config}")
        
        # åˆ›å»ºè§£æå™¨
        parser = WordNodeParser(
            chunk_size=docx_config.get("chunk_size", 1024),
            chunk_overlap=docx_config.get("chunk_overlap", 150)
        )
        
        print(f"âœ… è§£æå™¨åˆ›å»ºæˆåŠŸ")
        print(f"   chunk_size: {parser.chunk_size}")
        print(f"   chunk_overlap: {parser.chunk_overlap}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_word_chunking_with_mock_document():
    """ä½¿ç”¨æ¨¡æ‹Ÿæ–‡æ¡£æµ‹è¯•Wordåˆ‡åˆ†åŠŸèƒ½"""
    print("\nğŸ“„ æµ‹è¯•Wordæ–‡æ¡£åˆ‡åˆ†åŠŸèƒ½...")
    
    try:
        from llama_index.core.schema import Document
        from src.node_parser.word_parser import WordNodeParser
        from config.config_rag import CHUNKING_CONFIG
        
        # åˆ›å»ºä¸€ä¸ªé•¿æ–‡æœ¬æ¨¡æ‹ŸWordæ–‡æ¡£å†…å®¹
        long_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚" * 500  # å¤§çº¦7000å­—ç¬¦
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ–‡æ¡£
        mock_doc = Document(
            text=long_text,
            metadata={
                'file_name': 'test.docx',
                'file_type': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
            }
        )
        
        print(f"ğŸ“‹ æ¨¡æ‹Ÿæ–‡æ¡£ä¿¡æ¯:")
        print(f"   é•¿åº¦: {len(long_text)} å­—ç¬¦")
        print(f"   æ–‡ä»¶å: {mock_doc.metadata['file_name']}")
        
        # è·å–é…ç½®
        config = CHUNKING_CONFIG.get("docx", CHUNKING_CONFIG["default"])
        print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {config}")
        
        # åˆ›å»ºè§£æå™¨
        parser = WordNodeParser(
            chunk_size=config["chunk_size"],
            chunk_overlap=config["chunk_overlap"]
        )
        
        # æ‰§è¡Œåˆ‡åˆ†
        nodes = parser.get_nodes_from_documents([mock_doc])
        
        print(f"\nğŸ“Š åˆ‡åˆ†ç»“æœ:")
        print(f"   ç”ŸæˆèŠ‚ç‚¹æ•°: {len(nodes)}")
        
        # åˆ†ææ¯ä¸ªèŠ‚ç‚¹
        total_length = 0
        for i, node in enumerate(nodes):
            content_length = len(node.get_content())
            total_length += content_length
            print(f"   èŠ‚ç‚¹ {i+1}: {content_length} å­—ç¬¦")
            if hasattr(node, 'metadata') and 'parser_type' in node.metadata:
                print(f"      è§£æå™¨ç±»å‹: {node.metadata['parser_type']}")
        
        print(f"   æ€»é•¿åº¦: {total_length} å­—ç¬¦")
        print(f"   é¢„æœŸèŠ‚ç‚¹æ•°: {len(long_text) // config['chunk_size'] + 1}")
        
        # éªŒè¯æ˜¯å¦æ­£ç¡®åˆ‡åˆ†
        expected_chunks = max(1, len(long_text) // config["chunk_size"])
        if len(nodes) >= expected_chunks:
            print("âœ… åˆ‡åˆ†åŠŸèƒ½æ­£å¸¸")
            return True
        else:
            print(f"âŒ åˆ‡åˆ†å¼‚å¸¸ï¼šæœŸæœ›è‡³å°‘ {expected_chunks} ä¸ªèŠ‚ç‚¹ï¼Œå®é™…å¾—åˆ° {len(nodes)} ä¸ª")
            return False
            
    except Exception as e:
        print(f"âŒ Wordåˆ‡åˆ†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_actual_document_chunking():
    """æµ‹è¯•å®é™…æ–‡æ¡£çš„åˆ‡åˆ†æƒ…å†µ"""
    print("\nğŸ“‚ æµ‹è¯•å®é™…æ–‡æ¡£åˆ‡åˆ†...")
    
    try:
        from src.indices.index import load_nodes_from_disk
        from src.node_parser.node_parser_tool import get_parser_for_document
        
        # åŠ è½½å®é™…æ–‡æ¡£
        docs = load_nodes_from_disk()
        if not docs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å®é™…æ–‡æ¡£")
            return False
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(docs)} ä¸ªåŸå§‹æ–‡æ¡£")
        
        for i, doc in enumerate(docs):
            print(f"\nğŸ“„ æ–‡æ¡£ {i+1}:")
            print(f"   ID: {doc.id_}")
            print(f"   é•¿åº¦: {len(doc.get_content())} å­—ç¬¦")
            
            # æ£€æŸ¥å…ƒæ•°æ®
            if hasattr(doc, 'metadata') and doc.metadata:
                file_name = doc.metadata.get('file_name', 'Unknown')
                print(f"   æ–‡ä»¶å: {file_name}")
                
                # è·å–å¯¹åº”çš„è§£æå™¨
                parser = get_parser_for_document(doc)
                print(f"   è§£æå™¨ç±»å‹: {type(parser).__name__}")
                print(f"   chunk_size: {getattr(parser, 'chunk_size', 'N/A')}")
                print(f"   chunk_overlap: {getattr(parser, 'chunk_overlap', 'N/A')}")
                
                # å°è¯•é‡æ–°åˆ‡åˆ†
                try:
                    nodes = parser.get_nodes_from_documents([doc])
                    print(f"   é‡æ–°åˆ‡åˆ†ç»“æœ: {len(nodes)} ä¸ªèŠ‚ç‚¹")
                    
                    # æ˜¾ç¤ºå‰3ä¸ªèŠ‚ç‚¹çš„é•¿åº¦
                    for j, node in enumerate(nodes[:3]):
                        content_length = len(node.get_content())
                        print(f"      èŠ‚ç‚¹ {j+1}: {content_length} å­—ç¬¦")
                        
                except Exception as parse_error:
                    print(f"   âŒ åˆ‡åˆ†å¤±è´¥: {parse_error}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å®é™…æ–‡æ¡£æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_node_parser_directly():
    """ç›´æ¥æµ‹è¯•SimpleNodeParserçš„åŸºç¡€åŠŸèƒ½"""
    print("\nğŸ”§ æµ‹è¯•SimpleNodeParseråŸºç¡€åŠŸèƒ½...")
    
    try:
        from llama_index.core.node_parser import SimpleNodeParser
        from llama_index.core.schema import Document
        
        # åˆ›å»ºé•¿æ–‡æœ¬
        long_text = "è¿™æ˜¯æµ‹è¯•æ®µè½ã€‚" * 1000  # çº¦8000å­—ç¬¦
        
        # åˆ›å»ºæ–‡æ¡£
        doc = Document(text=long_text)
        
        # åˆ›å»ºSimpleNodeParser
        parser = SimpleNodeParser(
            chunk_size=1200,
            chunk_overlap=150
        )
        
        print(f"ğŸ“‹ æµ‹è¯•å‚æ•°:")
        print(f"   æ–‡æ¡£é•¿åº¦: {len(long_text)} å­—ç¬¦")
        print(f"   chunk_size: {parser.chunk_size}")
        print(f"   chunk_overlap: {parser.chunk_overlap}")
        
        # æ‰§è¡Œåˆ‡åˆ†
        nodes = parser.get_nodes_from_documents([doc])
        
        print(f"\nğŸ“Š åˆ‡åˆ†ç»“æœ:")
        print(f"   ç”ŸæˆèŠ‚ç‚¹æ•°: {len(nodes)}")
        
        for i, node in enumerate(nodes[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            content_length = len(node.get_content())
            print(f"   èŠ‚ç‚¹ {i+1}: {content_length} å­—ç¬¦")
        
        # éªŒè¯
        expected_nodes = len(long_text) // 1200 + 1
        if len(nodes) >= expected_nodes // 2:  # å…è®¸ä¸€å®šè¯¯å·®
            print("âœ… SimpleNodeParseråŸºç¡€åŠŸèƒ½æ­£å¸¸")
            return True
        else:
            print(f"âŒ åˆ‡åˆ†ç»“æœå¼‚å¸¸")
            return False
            
    except Exception as e:
        print(f"âŒ SimpleNodeParseræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹Wordæ–‡æ¡£åˆ‡åˆ†åŠŸèƒ½æ’æŸ¥")
    print("="*60)
    
    # æ¿€æ´»condaç¯å¢ƒæç¤º
    print("âš ï¸ è¯·ç¡®ä¿å·²æ¿€æ´» llamaindex ç¯å¢ƒï¼šconda activate llamaindex")
    print()
    
    success_count = 0
    total_tests = 4
    
    # æµ‹è¯•1ï¼šé…ç½®æ£€æŸ¥
    if test_word_parser_configuration():
        success_count += 1
    
    # æµ‹è¯•2ï¼šæ¨¡æ‹Ÿæ–‡æ¡£åˆ‡åˆ†
    if test_word_chunking_with_mock_document():
        success_count += 1
    
    # æµ‹è¯•3ï¼šå®é™…æ–‡æ¡£åˆ‡åˆ†
    if test_actual_document_chunking():
        success_count += 1
    
    # æµ‹è¯•4ï¼šSimpleNodeParseråŸºç¡€åŠŸèƒ½
    if test_simple_node_parser_directly():
        success_count += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   æˆåŠŸ: {success_count}/{total_tests}")
    print(f"   å¤±è´¥: {total_tests - success_count}/{total_tests}")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Wordåˆ‡åˆ†åŠŸèƒ½æ­£å¸¸ï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼ŒWordåˆ‡åˆ†åŠŸèƒ½å­˜åœ¨é—®é¢˜") 