"""
æœ€ç®€å•çš„èŠ‚ç‚¹è¿”å›åŠŸèƒ½éªŒè¯
"""
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

def test_basic_document_loading():
    """æµ‹è¯•åŸºç¡€æ–‡æ¡£åŠ è½½åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•åŸºç¡€æ–‡æ¡£åŠ è½½...")
    
    try:
        from src.indices.index import load_nodes_from_disk
        
        # ç›´æ¥ä»ç£ç›˜åŠ è½½èŠ‚ç‚¹
        docs = load_nodes_from_disk()
        
        if docs is None:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æŒä¹…åŒ–çš„æ–‡æ¡£èŠ‚ç‚¹")
            return False
        
        if len(docs) == 0:
            print("âŒ æ–‡æ¡£èŠ‚ç‚¹åˆ—è¡¨ä¸ºç©º")
            return False
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£èŠ‚ç‚¹")
        
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„åŸºæœ¬ä¿¡æ¯
        first_doc = docs[0]
        print(f"\nğŸ“‹ ç¬¬ä¸€ä¸ªæ–‡æ¡£èŠ‚ç‚¹ä¿¡æ¯:")
        print(f"   ç±»å‹: {type(first_doc)}")
        print(f"   ID: {getattr(first_doc, 'id_', 'N/A')}")
        print(f"   èŠ‚ç‚¹ID: {getattr(first_doc, 'node_id', 'N/A')}")
        
        # è·å–å†…å®¹é•¿åº¦
        if hasattr(first_doc, 'get_content'):
            content = first_doc.get_content()
            print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   å†…å®¹é¢„è§ˆ: {content[:200]}...")
        elif hasattr(first_doc, 'text'):
            text = first_doc.text
            print(f"   æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            print(f"   æ–‡æœ¬é¢„è§ˆ: {text[:200]}...")
        
        # æ£€æŸ¥å…ƒæ•°æ®
        if hasattr(first_doc, 'metadata') and first_doc.metadata:
            print(f"   å…ƒæ•°æ®: {first_doc.metadata}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡æ¡£åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_document_content_analysis():
    """åˆ†ææ–‡æ¡£å†…å®¹ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨é•¿åº¦é—®é¢˜"""
    print("\nğŸ“Š åˆ†ææ–‡æ¡£å†…å®¹...")
    
    try:
        from src.indices.index import load_nodes_from_disk
        
        docs = load_nodes_from_disk()
        if not docs:
            print("âŒ æ²¡æœ‰æ–‡æ¡£å¯åˆ†æ")
            return False
        
        print(f"ğŸ“„ åˆ†æ {len(docs)} ä¸ªæ–‡æ¡£èŠ‚ç‚¹:")
        
        total_length = 0
        long_docs = 0
        empty_docs = 0
        
        for i, doc in enumerate(docs):
            # è·å–å†…å®¹
            content = ""
            if hasattr(doc, 'get_content'):
                content = doc.get_content()
            elif hasattr(doc, 'text'):
                content = doc.text
            elif hasattr(doc, 'content'):
                content = doc.content
            
            content_length = len(content)
            total_length += content_length
            
            if content_length == 0:
                empty_docs += 1
                print(f"   âš ï¸ æ–‡æ¡£ {i+1}: å†…å®¹ä¸ºç©º")
            elif content_length > 2048:
                long_docs += 1
                print(f"   ğŸ“ æ–‡æ¡£ {i+1}: {content_length} å­—ç¬¦ (è¶…è¿‡2048é™åˆ¶)")
            else:
                print(f"   âœ… æ–‡æ¡£ {i+1}: {content_length} å­—ç¬¦ (ç¬¦åˆè¦æ±‚)")
        
        print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"   æ€»æ–‡æ¡£æ•°: {len(docs)}")
        print(f"   å¹³å‡é•¿åº¦: {total_length // len(docs)} å­—ç¬¦")
        print(f"   ç©ºæ–‡æ¡£æ•°: {empty_docs}")
        print(f"   è¶…é•¿æ–‡æ¡£æ•°: {long_docs}")
        print(f"   åˆæ ¼æ–‡æ¡£æ•°: {len(docs) - empty_docs - long_docs}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å†…å®¹åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_retrieval_without_embedding():
    """æµ‹è¯•ä¸ä¾èµ–åµŒå…¥çš„ç®€å•æ£€ç´¢"""
    print("\nğŸ” æµ‹è¯•ç®€å•æ–‡æœ¬åŒ¹é…æ£€ç´¢...")
    
    try:
        from src.indices.index import load_nodes_from_disk
        from llama_index.core.schema import NodeWithScore
        
        docs = load_nodes_from_disk()
        if not docs:
            print("âŒ æ²¡æœ‰æ–‡æ¡£å¯æ£€ç´¢")
            return False
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        query = "HCSL"
        print(f"ğŸ” æœç´¢å…³é”®è¯: {query}")
        
        matched_nodes = []
        
        for doc in docs:
            # è·å–å†…å®¹
            content = ""
            if hasattr(doc, 'get_content'):
                content = doc.get_content()
            elif hasattr(doc, 'text'):
                content = doc.text
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            if query.lower() in content.lower():
                # è®¡ç®—ç®€å•çš„åŒ¹é…åˆ†æ•°ï¼ˆå…³é”®è¯å‡ºç°æ¬¡æ•°ï¼‰
                score = content.lower().count(query.lower()) / len(content.split())
                
                # åˆ›å»ºNodeWithScoreå¯¹è±¡
                node_with_score = NodeWithScore(node=doc, score=score)
                matched_nodes.append(node_with_score)
        
        # æŒ‰åˆ†æ•°æ’åº
        matched_nodes.sort(key=lambda x: x.score, reverse=True)
        
        print(f"âœ… æ‰¾åˆ° {len(matched_nodes)} ä¸ªåŒ¹é…çš„èŠ‚ç‚¹")
        
        # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
        for i, node_with_score in enumerate(matched_nodes[:3], 1):
            node = node_with_score.node
            content = node.get_content() if hasattr(node, 'get_content') else str(node)
            
            print(f"\nğŸ“‹ åŒ¹é…èŠ‚ç‚¹ {i}:")
            print(f"   åˆ†æ•°: {node_with_score.score:.4f}")
            print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            print(f"   å†…å®¹é¢„è§ˆ: {content[:150]}...")
        
        return len(matched_nodes) > 0
        
    except Exception as e:
        print(f"âŒ ç®€å•æ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_node_display_functionality():
    """æµ‹è¯•èŠ‚ç‚¹æ˜¾ç¤ºåŠŸèƒ½"""
    print("\nğŸ¨ æµ‹è¯•èŠ‚ç‚¹æ˜¾ç¤ºåŠŸèƒ½...")
    
    try:
        from src.indices.index import load_nodes_from_disk
        from src.utils.node_display import display_retrieved_nodes, NodeDisplayer
        from llama_index.core.schema import NodeWithScore
        
        docs = load_nodes_from_disk()
        if not docs:
            print("âŒ æ²¡æœ‰æ–‡æ¡£å¯æ˜¾ç¤º")
            return False
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„NodeWithScoreåˆ—è¡¨
        mock_nodes = []
        for i, doc in enumerate(docs[:2]):  # åªå–å‰2ä¸ªæ–‡æ¡£
            score = 0.8 - i * 0.1  # æ¨¡æ‹Ÿåˆ†æ•°
            node_with_score = NodeWithScore(node=doc, score=score)
            mock_nodes.append(node_with_score)
        
        print(f"âœ… åˆ›å»ºäº† {len(mock_nodes)} ä¸ªæ¨¡æ‹ŸèŠ‚ç‚¹")
        
        # æµ‹è¯•æ˜¾ç¤ºåŠŸèƒ½
        display_retrieved_nodes(
            mock_nodes, 
            query="æµ‹è¯•æŸ¥è¯¢", 
            show_summary=True
        )
        
        return True
        
    except Exception as e:
        print(f"âŒ èŠ‚ç‚¹æ˜¾ç¤ºæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æœ€ç®€å•çš„èŠ‚ç‚¹è¿”å›åŠŸèƒ½éªŒè¯")
    print("="*60)
    
    # æ¿€æ´»condaç¯å¢ƒæç¤º
    print("âš ï¸ è¯·ç¡®ä¿å·²æ¿€æ´» llamaindex ç¯å¢ƒï¼šconda activate llamaindex")
    print()
    
    success_count = 0
    total_tests = 4
    
    # æµ‹è¯•1ï¼šåŸºç¡€æ–‡æ¡£åŠ è½½
    if test_basic_document_loading():
        success_count += 1
    
    # æµ‹è¯•2ï¼šæ–‡æ¡£å†…å®¹åˆ†æ
    if test_document_content_analysis():
        success_count += 1
    
    # æµ‹è¯•3ï¼šç®€å•æ£€ç´¢ï¼ˆä¸ä½¿ç”¨åµŒå…¥ï¼‰
    if test_simple_retrieval_without_embedding():
        success_count += 1
    
    # æµ‹è¯•4ï¼šèŠ‚ç‚¹æ˜¾ç¤ºåŠŸèƒ½
    if test_node_display_functionality():
        success_count += 1
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   æˆåŠŸ: {success_count}/{total_tests}")
    print(f"   å¤±è´¥: {total_tests - success_count}/{total_tests}")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼èŠ‚ç‚¹è¿”å›åŠŸèƒ½éªŒè¯æˆåŠŸï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½") 